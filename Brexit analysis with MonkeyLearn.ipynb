{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brexit Twitter Analysis\n",
    "\n",
    "### First, set in the environment your Twitter credentials and MonkeyLearn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TWITTER_CONSUMER_KEY = os.environ.get('TWITTER_CONSUMER_KEY')\n",
    "TWITTER_CONSUMER_SECRET = os.environ.get('TWITTER_CONSUMER_SECRET')\n",
    "TWITTER_ACCESS_KEY = os.environ.get('TWITTER_ACCESS_KEY')\n",
    "TWITTER_ACCESS_SECRET = os.environ.get('TWITTER_ACCESS_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, let's just download them!\n",
    "In *tweets_quantity*, set the amount of tweets you'll download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag = '#Brexit'\n",
    "tweets_file_name = 'Brexit_tweets.csv'\n",
    "tweets_quantity = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from time import clock, sleep\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "auth = tweepy.OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)\n",
    "auth.set_access_token(TWITTER_ACCESS_KEY, TWITTER_ACCESS_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "start = clock()\n",
    "with open(tweets_file_name, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    class StreamListener(tweepy.StreamListener):\n",
    "\n",
    "        collected_tweets = 0\n",
    "\n",
    "        def on_status(self, status):\n",
    "            try:\n",
    "                tweet = status.text\n",
    "                tweet = tweet.replace('\\n', '\\\\n')\n",
    "                timePass = clock() - start\n",
    "                if timePass % 60 == 0:\n",
    "                    print (\"I have been working for\", timePass, \"seconds.\")\n",
    "                if not ('RT @' in tweet):  # Exclude re-tweets\n",
    "                    writer.writerow([tweet])\n",
    "                    self.collected_tweets += 1\n",
    "                    if self.collected_tweets % 1000 == 0:\n",
    "                        print (\"I have collected\", self.collected_tweets, \"tweets!\")\n",
    "                    if self.collected_tweets == tweets_quantity:\n",
    "                        print (\"I have finished!\")\n",
    "                        return False\n",
    "                    pass\n",
    "\n",
    "            except Exception as e:\n",
    "                sys.stderr.write('Encountered Exception:' + str(e))\n",
    "                pass\n",
    "\n",
    "        def on_error(self, status_code):\n",
    "            print('Error: ' + repr(status_code))\n",
    "            return True  # False to stop\n",
    "\n",
    "        def on_delete(self, status_id, user_id):\n",
    "            \"\"\"Called when a delete notice arrives for a status\"\"\"\n",
    "            print(\"Delete notice for\" + str(status_id) + '. ' + str(user_id))\n",
    "            return\n",
    "\n",
    "        def on_limit(self, track):\n",
    "            \"\"\"Called when a limitation notice arrives\"\"\"\n",
    "            return\n",
    "\n",
    "        def on_timeout(self):\n",
    "            \"\"\"Called when there is a timeout\"\"\"\n",
    "            sys.stderr.write('Timeout...')\n",
    "            sleep(10)\n",
    "            return True\n",
    "\n",
    "    streamingAPI = tweepy.streaming.Stream(auth, StreamListener())\n",
    "    streamingAPI.filter(track=[hashtag])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make sentiment analysis on the tweets, only if they are in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from monkeylearn import MonkeyLearn\n",
    "\n",
    "MONKEYLEARN_API_KEY = os.environ.get('MONKEYLEARN_API_KEY')\n",
    "ml = MonkeyLearn(MONKEYLEARN_API_KEY)\n",
    "\n",
    "module_id = 'pi_SyZF3Kje' # This is the id of the pipeline that we are using\n",
    "\n",
    "tweets = []\n",
    "\n",
    "chunk_size = min(500, limit)\n",
    "chunk_count, count = 0, 0\n",
    "chunk = []\n",
    "\n",
    "with open(tweets_file_name, 'r') as f:\n",
    "    for row in csv.reader(f):\n",
    "        chunk.append(row)\n",
    "        count += 1\n",
    "        chunk_count += 1\n",
    "        if chunk_count == chunk_size:\n",
    "            data = {\n",
    "                \"texts\": [{\"text\": sample[0]} for sample in chunk]\n",
    "            }\n",
    "            res = ml.pipelines.run(module_id, data)\n",
    "            i = 0\n",
    "            for d in res.result['results']:\n",
    "                if d['lang'][0][\"label\"] == \"English\" and d['lang'][0][\"probability\"] > 0.6:\n",
    "                    tweets.append({\"text\": chunk[i][0], \"sentiment\": d[\"sentiment_tweet\"][0]})\n",
    "                i += 1\n",
    "            chunk = []\n",
    "            chunk_count = 0\n",
    "print('Total tweets:', count)\n",
    "print('Total tweets in English:', len(tweets))\n",
    "positive_tweets = [tweet for tweet in tweets if tweet['sentiment']['label'] == 'positive']\n",
    "print('Positive tweets:', len(positive_tweets))\n",
    "negative_tweets = [tweet for tweet in tweets if tweet['sentiment']['label'] == 'negative']\n",
    "print('Negative tweets:', len(negative_tweets))\n",
    "neutral_tweets = [tweet for tweet in tweets if tweet['sentiment']['label'] == 'neutral']\n",
    "print('Neutral tweets:', len(neutral_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's extract the keywords for each of the three categories. You'll get the 10 most relevant for each.\n",
    "We'll take the first *sample_size* tweets for each category, join them in one text and extract the keywords with MonkeyLearn. If *sample_size* is too big, the lenght of the text may fail because it may reach the lenght limit for your plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_size = 5000\n",
    "\n",
    "values = {\n",
    "    \"negative\":{\"found\":0, \"text\":\"\"},\n",
    "    \"neutral\":{\"found\":0, \"text\":\"\"},\n",
    "    \"positive\":{\"found\":0, \"text\":\"\"}\n",
    "}\n",
    "\n",
    "for tweet in tweets:\n",
    "    sent = tweet[\"sentiment\"]\n",
    "    if sent[\"probability\"] > 0.6:\n",
    "        if values[sent[\"label\"]][\"found\"] < sample_size:\n",
    "            values[sent[\"label\"]][\"text\"] += \"\\n\" + tweet[\"text\"]\n",
    "            values[sent[\"label\"]][\"found\"] += 1\n",
    "\n",
    "    if values[\"negative\"][\"found\"] >= sample_size and values[\"neutral\"][\"found\"] >= sample_size and values[\"positive\"][\"found\"] >= sample_size:\n",
    "        break\n",
    "\n",
    "module_id = 'ex_y7BPYzNG' # This is the id of the keyword extractor\n",
    "\n",
    "for sentName, sentDict in values.items():\n",
    "    print(sentName keywords: )\n",
    "    print()\n",
    "    res = ml.extractors.extract(module_id, [sentDict[\"text\"]])\n",
    "    for d in res.result[0]:\n",
    "        print(d[\"keyword\"])\n",
    "    sentDict[\"keywords\"] = res.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's get the sentiment values for the tweets where they mention some words.\n",
    "#### Fell free to play with the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these are the keywords, you can add more\n",
    "counts = {\n",
    "    \"trump\":{},\n",
    "    \"cameron\":{},\n",
    "    \"scotland\":{},\n",
    "    \"democracy\":{},\n",
    "    \"nigel farage\":{}\n",
    "}\n",
    "\n",
    "for itemName, itemDict in counts.items():\n",
    "    itemDict[\"positive\"] = 0\n",
    "    itemDict[\"neutral\"] = 0\n",
    "    itemDict[\"negative\"] = 0\n",
    "    \n",
    "for tweet in tweets:\n",
    "    for keyName, keyDict in counts.items():\n",
    "        if keyName in tweet[\"text\"].lower():\n",
    "            keyDict[tweet[\"sentiment\"][\"label\"]] += 1\n",
    "\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
